[
    {
        "name": "FosteredFood",
        "projectTitle": "The FosteredFood Application: Cohesion of Mutual Aid Networks with Support from Technological Integration",
        "projectOverview": "FosteredFood is a community-driven web application project developed in collaboration with local community fridges, with the primary goal of mitigating concerns related to food access and food sovereignty. This initiative involved the development of both software and low-cost temperature sensors, which were sought to be deployed to enhance volunteer coordination, ensure food safety in the fridges, and share critical information about fridge contents with the public. The project introduced both the software and hardware alongside qualitative fieldwork to evaluate the effectiveness, adoption, and expected impact of the web application on the involved communities.",
        "githubLink": "https://github.com/taylor-stevens/fostered-food",
        "websiteLink": "https://taylor-stevens.github.io/fostered-food/",
        "websiteLinkDescription": "active website",
        "projectImagePreviewSrc": "./dist/assets/fosteredfood.jpg",
        "toolIcons": ["fa-brands fa-react", "fa-brands fa-js", "fa-brands fa-node-js", "fa-brands fa-fly", "fa-brands fa-docker", "fa-brands fa-square-google-plus", "fa-brands fa-bootstrap", "fa-solid fa-leaf", "fa-solid fa-magnifying-glass"],
        "toolImages": ["./src/assets/language-typescript.png"],
        "leftDescriptionTitle": "Software",
        "leftDescriptionInfo": "\nAt the heart of the project, the software framework revolved around React and Node.js as the primary Javascript library and environment for the application. React was chosen for its ability to create reusable UI components and efficiently update the user interface based on real-time data changes without necessitating a complete page refresh. Coupled with Node.js, this allowed for a consistent Javascript and Typescript ecosystem throughout the codebase. For the backend deployment, the project employed fly.io in conjunction with Docker. This choice was informed by its rapid configurability, simplified image building from Dockerfiles, and user-friendly command line prompts, culminating in a hassle-free deployment process. Furthermore, Fly.io's free accessibility aligns with the primary goals of our project.\n\nThe database for the application relies on Google Sheets and the accompanying Google APIs. This selection is motivated by its visibility, transparency, and user-friendliness for non-technical audiences. Further, the Google Suite is accessible through multiple means including public web browsers at local libraries as well as numerous personal devices, extending the accessibility of the project significantly. Implementing async functions in the APIs enhances the application's efficiency and responsiveness, reducing the risk of blocking code. Lastly, the repository uses the dotenv package to make the configuration-related information more secure and organized.\n\nThe components of the application are built using React's functional components, enhancing simplicity, consistency, and reusability. React Hooks API, including useEffect for updating elements and managing information via contexts, is instrumental in streamlining the user experience. Within the functional components, the React Hook’s API is used not only to help update elements (useEffect) and to update the map, but also to easily keep track of information via contexts (useContext).\n\nAlongside React’s Components and Hooks, the project also implements React’s Context Providers to propagate data to children components. Context Providers play a crucial role in propagating data to child components, simplifying state management, cross-component communication, and enhancing reusability. This made it easier to maintain simplicity in the code when the data was fetched from the backend. It was also important in managing the global state and sharing the data and functionality across the application without passing props down through multiple layers of components, also known as prop drilling. Overall, the Contexts simplified state management while maintaining successful cross-component communication. Without prop drilling, the application further became more readable and the components were easier to re-use. Lastly, the Context Providers simplified testing components with the ability to provide mock contexts that eliminated test dependency concerns.\n\nAlongside React’s features, Bootstrap is heavily employed for its consistent design, responsive layout, cross-browser compatibility, and accessibility features. It aids in rendering placeholder components and user notifications. In addition to this, it aids in reducing the need for time and resources surrounding front-end development. One way that it was used was for its placeholder components which allow for alternative rendering in situations where async functions have either not yet returned or have failed, as well as for user notifications.\n\nFor the Map rendering in the application, there were many tile providers to choose from. For this project, we choose the Transport Map via the OpenStreetMap Tile provider. ‘OpenStreetMap is a free, editable map of the globe that is built by volunteers largely from scratch and released with an open-content license’. This tile provider works well with Leaflet, ‘a modern open-source JavaScript library for mobile-friendly interactive map[s]’, which met the requirements of the project scope which included working well with NodeJS and React and allowing for interactivity within the application. Requests to the Tile Provider are made with the TileLayer react-leaflet library The Leaflet library further allows for easy user interaction such as panning over and zooming in on the map tiles, which are both vital in user experience.\n\n",
        "rightDescriptionTitle": "Hardware",
        "rightDescriptionInfo": "\nOne of the main hardware requirements included the translation and transmission of temperature data via TMP36 Temperature Sensors from Analog Devices to a publicly available Google Sheets. These sensors were chosen as the test sensors were able to be donated, and in large batches they are a very cheap part. Further, the sensors provide a wide range of temperature readings that are highly accurate and can be used in both refrigerators and freezers.\n\nIn the test sensors, the HiLetgo ESP32 LoRa WiFi Arduino Boards were used in conjunction with 3.7V Rechargeable Lithium Batteries. Multiple of these batteries can be attached in parallel to multiply the mAh availability, and they are also easily removable for recharging, making it a good long term and cheap solution. The board was specifically chosen to help keep costs low and the batteries were chosen such that they could be reused and so that the boards did not have to have a connection to power outside of the fridge. Further, these boards come equipped with WiFi transmission if the LoRa transmission were to fail, which allows for backup since most of the community fridges are housed by restaurants (or stores) with public WiFi. By keeping modifications to existing structures minimal, the project would be feasible for multiple mutual aid groups in the community fridge network.\n\nThe data collected by the temperature sensor would be collected by the board and transmitted via LoRa which requires less power over time, extending the shelf life of the temperature sensor boards. The data that is populating the Web Application is currently being held in Google Sheets as a database, which is where the sensor’s data would be pushed onto in real time were the boards to be installed. The last piece of the hardware required for successful installation is the enclosure that will protect it from moisture, extreme temperatures, and small physical damage from movement. After a few tests on our team, the enclosures could be easily 3D printed via a Computer Aided Design software, maintaining low cost of the project. "
    },
    {
        "name": "Addressing Political Bias",
        "projectTitle": "Addressing Political Bias in News Articles with Multinomial Regression",
        "projectOverview": "The objective of the project centers on the optimization of a multinomial regression model (MLRM) tailored for predicting bias in online articles. It is hypothesized that although political bias is a complex problem, a MLRM will be able to classify a satisfiable amount of articles that it is presented with. This model operates on a dataset consisting of crucial columns: topic, source, bias_score, and paragraph_vectors (reflecting article content), each numerically encoded and normalized. The optimization would include elevating the model's efficacy and ensuring dependable predictions when confronted with unseen articles. In general, the multinomial regression model is known for its ability to predict probabilities of group membership across multiple classes or categories. In the case of this project, it is tuned to recognize and predict the bias category of online articles, drawing insights from the features inherent in the dataset columns. Across multiple iterations, the model provides probability estimates to articles such that they are in various bias categories, with the goal of reducing error as it trains itself. The three categories that this project's model is training to predict includes both left, center, and right labels, translated into the numerical scores of 0, 1, and 2 respectively. Each score showcases the dominant bias ingrained within the article based on the provided features. Aside from the goal of having a high accuracy rate in classifying articles, it is important to understand the model’s output and the sources of misclassification. ",
        "githubLink": "https://github.com/anjali-tanna/cs4100_final_project",
        "websiteLink": "https://docs.google.com/document/d/1Ikoay_N5WoR-IqjMdaVAHz-y5ivq-MUC452NJHw-rLk/edit#heading=h.xxfojxegu9p5",
        "websiteLinkDescription": "Final Report",
        "projectImagePreviewSrc": "./src/assets/mlrm_overview.jpg",
        "toolIcons": ["fa-brands fa-python"],
        "toolImages": ["./src/assets/numpy.png", "./src/assets/pandas.png", "./src/assets/scikit-learn.png"],
        "leftDescriptionTitle": "Database Selection and Cleaning",
        "leftDescriptionInfo": "\n The dataset utilized for training and testing the model is sourced from a publicly available repository on Google Datasets, accessible through the Hugging Face platform. Composed of 13 comprehensive columns, including important attributes such as topic, source, bias, url, title, date, authors, content, source_url, and bias_text, this dataset stands as a foundation for the analysis to be undertaken post training. The project's success heavily relied upon pre-labeled data, which this repository was able to provide. Without the availability of labeled data, the model would have been performing unsupervised learning, which would have introduced ambiguity and complexity, potentially undermining the analysis. The supervised learning approach that was taken ensured a more straightforward trajectory in discerning bias classifications, particularly in the political spectrum. Furthermore, the existence of labeled data allowed for rigorous testing, validating the accuracy and efficacy of the classification model. Nuance was imperative for training a MLRM as generally these types of models are not the strongest available to classify complex ideas. The dataset's bias distribution represents a broad spectrum of the ways in which bias can permeate articles. Understanding the dataset’s distributions proved instrumental in understanding the model's sensitivity to different biases. Additionally, examining bias splits by source shed light on the inherent biases affiliated with specific platforms or publishers, crucial insights that guided the understanding of how biases manifest across diverse sources. \n\n Upon acquiring the dataset, a series of preprocessing measures were undertaken to ensure data coherence and relevance to the classification objectives. The primary aim was to refine the dataset, making it more comprehensible for the model while eliminating extraneous or potentially misleading attributes. Initially, columns prone to introducing noise or misleading the model's learning process, such as author, date, time, etc. were removed. This curation process aided in reducing the dataset down to attributes appropriate to the classification goals. As a result, topic, source, bias, title, and content, stood as the remaining columns in the dataset for the model to train with. After column reduction was complete, title and content were transformed by Doc2Vec, a powerful vectorization library. The utilization of Doc2Vec allowed for the mapping of textual data to meaningful numerical representations. The two columns were merged before undergoing the transformation by Doc2Vec, which empowered the model to capture not just word importance but the semantic meaning of the entire article including its title. Further, from this merged column, prior to applying the library, filler words, commonly known as stop words, were removed from the content. This extraction process aimed to reveal the essence of the articles, eliminating linguistic clutter that might obscure the model's understanding. This nuanced representation resulted in the creation of an output array comprising 100 feature values per data sample. The utilization of Doc2Vec, which pivoted away from simple word importance, enabled the model to grasp the intricate nuances and contextual depth embedded within each article. This was important as bias cannot simply be captured by which words an author may have chosen. After the vectorization of the text was completed, the source and topic columns underwent separate preprocessing. Each column was given its own manual vectorization, replacing word values with numerical values through mapping. Finally, the 3 vectorized columns (source, topic, and the merged content/title) underwent normalization, ensuring uniformity and consistency in scale across the feature space, a crucial step preceding model training. Prior to introducing this step, the model had significant trouble with classifying the articles, likely due to the different scales between source/topic and content/title. This sequence of preprocessing steps, ranging from column curation to semantic representation and normalization, resulted in a refined dataset optimized for the model's comprehension. By leveraging these techniques to consolidate information, the preprocessing allowed for the model training to result in significantly more accurate bias classification than before.",
        "rightDescriptionTitle": "Analysis and Discussion",
        "rightDescriptionInfo": "\nThe exploration of a multilogistic regression model (MLRM) surfaced considerable insights into the intricate nature of identifying political bias within written articles. Despite considerable progress made through a significant learning curve, it became evident that MLRM might possess inherent limitations in capturing the nuanced intricacies of this classification process, a barrier not predicted with the hypothesis. Although there was much more success after a significant learning curve, there are notable missing pieces in the project. One of the key limitations stemmed from MLRM's assumption of linear decision boundaries between classes. Political bias, however, can manifest as a multifaceted spectrum rather than adhering to distinctly separable linear boundaries. This complexity likely resulted in the model creating oversimplified classifications that failed to encapsulate the nuanced positions and ranges of political bias present in articles. Furthermore, MLRMs operate under the assumption of feature independence, a premise that might not hold true for political bias, which was not considered prior to analysis. This lack of feature independence could stem from not only language nuances and historical references but also contextual intricacies and rhetorical devices. Attempting to encapsulate these multifaceted aspects within a linear model framework would therefore pose significant challenges. Because MLRMs might struggle to contextualize these elements effectively, the training could result in a loss of crucial information essential for comprehending political leanings within articles. Considering these factors, while MLRM served as a valuable baseline model, achieving higher accuracies than found in this project would likely require more sophisticated approaches. Some techniques rooted in natural language processing (NLP), including deep learning architectures or ensemble methods, could likely better handle the complexity and non-linearity present in the data. Further NLP methods would likely find more success in capturing the subtle contextual cues vital for accurately identifying and classifying political bias within articles. Despite the model's inherent limitations, noteworthy insights emerged from the training. Notably, the selection of features such as source and topics significantly influenced the model's success, resulting in almost 85% accuracy in the classification of test data with just a little over 1000 samples in the dataset. Additionally, the utilization of the Doc2Vec library played a pivotal role in the model's ability to achieve satisfactory classification accuracy. The transformation of text based data into semantic representations through this library greatly contributed to the model's efficacy compared to other vectorization techniques that were tested. Without the DocToVec vectors as features, the model failed to even hit 50% classification abilities.\n\nAlongside the difficulties with MLRM and complex classification, several challenges and unmet expectations hindered the project's progression. The scarcity of data that precisely matched the desired criteria, specifically, the unavailability of a dataset with five categories for classification, imposed limitations on the model's complexity and accuracy. It is likely however, that although more complex data could not be found, that the MLRM would have struggled to classify a 5 category political bias, as the model's accuracy witnessed a slight decline from only 2 to 3 categories. Further, the scale of the dataset, though larger than manually curated alternatives, remained insufficient for practical applications. The limited volume of around 1,300 rows potentially impacted the model's accuracy due to inadequate instances for comprehensive learning. The search for a larger dataset proved difficult, consuming significant time without yielding the desired scale of data. When attempting to supplement the small dataset, it was quickly clear that it would not be reasonable to continue in the timeframe provided as it was extremely difficult to get raw text of articles due to paywalls and other cross origin resource policies (CORP) that blocked the use of libraries such as BeautifulSoup.\n\nAlong the way, some failed library usage atttempts, resulted in the discovery of Doc2Vec, which brought forth the realization that political bias might not merely be linked to word repetitions within articles. Rather, the identification of bias appears to be intricately linked to word relations, context, and broader linguistic nuances. This observation underscores the crucial role of context and word relationships in bias identification, deviating from a simplistic association with individual word occurrences."
    },
    {
        "name": "Covey-Town",
        "projectTitle": "Implementation of the \"Friends Feature\" within the Covey.Town World",
        "projectOverview": "All of the infomation found in this tab is referenced from the Implementation Report for this project. \n\n The primary feature that our team added to Covey.Town is Friends functionality: allowing players to become friends with each other. Once two players are friends, they can take advantage of the features we added that are offered exclusively for players with friends: teleporting, sending MiniMessages, and sending ConversationArea invites (herein collectively defined as “extended Friends features”). Initially, each player will see every other player in the town under the “Other Players In This Town” heading on the Social Sidebar. When a player gains a friend, the friend’s name is instead displayed under the “Friends” heading. ",
        "githubLink": "https://github.com/neu-cs4530/covey-town-friends-feature",
        "websiteLink": "https://covey-town-with-friends-group-308.netlify.app/",
        "websiteLinkDescription": "the original netlify site",
        "projectImagePreviewSrc": "./src/assets/coveytown.png",
        "toolIcons": ["fa-brands fa-react", "fa-brands fa-sass", "fa-brands fa-css3", "fa-brands fa-html5"],
        "toolImages": ["./src/assets/jest.png", "./src/assets/language-typescript.png"],
        "leftDescriptionTitle": "Technical Overview",
        "leftDescriptionInfo": "\n Most of the substantive changes to the existing Covey.Town codebase were made in Town.ts and TownController.ts. This is in part due to the main functionality of our features involving the addition of communication lines between players (i.e., TownControllers) in a given Town. To create these lines of communication, we closely paralleled the structure used for various events, such as interactableUpdate, prevalent in Individual Project 1 & 2. \n\n We started by adding new client-to-socket and socket-to-client events in CoveyTownSocket.ts, as well as new object types for these events to share information between the frontend and backend. We chose to prioritize reduction of transported data by sending only necessary information as opposed to reusing a singular broad event type. This design choice is reflected in our decision to create both a ConversationAreaGroupInvite and a TeleportInviteSingular object type. \n\n Once these were created, the bulk of the work was spent creating methods in Town.ts and TownController.ts to emit these events, and listeners in both of these classes to catch them. In TownController, we created methods corresponding to interactions a user could have in Covey.Town (such as sending a friend request), each of which emit corresponding client-to-server events (ex: sendFriendRequest) to the shared backend (Town.ts). In Town, we registered client-to-server event listeners under the addPlayer() method. The majority of these listeners call a corresponding Town method, which modifies backend data based on the provided information and then emits an equivalent socket-to-client event. In TownController, we registered listeners for these socket-to-client events under the registerSocketListeners() method, completing the frontend-backend-frontend loop. It should be noted that some of the listeners registered in Town.ts only emit the socket-to-client events, without actually performing backend changes. These events correspond to user actions that modify the user interfaces of at least two players without affecting any backend data (e.g., sending a friend request). Since TownControllers don’t have built-in direct communication with each other, going through the backend for these types of actions ensures that the socket-to-client events will be received by all players in the Town, and thus that all players involved in said action can propagate updates to their interfaces accordingly. \n\n The primary function of the listeners registered in TownController is to verify that the received event concerns said TownController’s user, and if it does, use the provided information to update one of four states: the TownController’s friends list, the TownController’s selected friends list, the TownController’s friend requests list, or the TownController’s conversation area invites list. The setters for each of these four states all emit a unique parameter-changed event, which is then caught by a corresponding React hook (defined in TownController.ts) to re-render the parameter. These hooks are then used in the frontend files to update the UI accordingly. The MiniMessage line of communication is implemented slightly differently, in that we didn’t create a parameter to update in TownController.ts - we simply emit a newMiniMessageReceived event in our listener for the miniMessageSent event. This event is then caught in the frontend (TownMap.tsx), along with the associated necessary information, to render the appropriate toast message on the corresponding players’ screen. \n\n Most of our changes to the UI involved modifying the Social Sidebar. It now first displays a list of the user’s friends, with associated teleport-to and unfriend buttons, followed by a button to invite selected friends to a conversation area, and a text box to write and send short messages (MiniMessages) to selected friends. In the next section, which displays the non-friends list (previously the town occupants list), users can now act via buttons to send, cancel, accept, or decline friend requests to other Town occupants. Additionally, with the help of a button in the active conversation areas section, a user can open a Chakra Drawer displaying all received conversation area invites, with the ability to accept or decline them. As another design choice, part of our refactoring involved modifying and renaming the PlayersList file to NonFriendsListArea, to prevent redundancy between the FriendsList we’d be displaying and the list of remaining players in the Town (ensuring mutual exclusivity between players and friends). Finally, App.css, ChatWindow.tsx, ParticipantList.tsx, and Room.tsx were also slightly modified in order to make the Social Sidebar respond to mouse events, and wrap the video area overlay so it wouldn't obscure any of our added features.",
        "rightDescriptionTitle": "Process Overview",
        "rightDescriptionInfo": "\n Using AGILE methodology, our group modeled a SCRUM team implementing a Kanban board on JIRA. We established early on the general process that we wanted to follow via weekly two-hour planning meetings, and revised this process throughout the project. For each meeting, we began with a quick stand-up to communicate progress statuses. The latter portions of the meetings were dedicated to collaborative work, such as completing reviews on outstanding pull requests (PRs), co-programming to help each other with any blockers regarding coding issues (such as failing tests, trouble implementing design, or difficulty with requirements), as well as doing planning and re-evaluation for the next sprint. \n\n We began with writing a Project Plan, in which we described and drafted visuals for our feature, identified user stories and conditions of satisfaction, and created an initial list outlining tasks to complete. These tasks started out relatively vague and incomplete as we did not have a great understanding of the codebase at that time. The initial high-level tasks were then added to the JIRA board, and revised as more detailed planning occurred each sprint to include specific requirements. Each task had a “lead” and a backup programmer assigned to them. These assignments were also re-evaluated during our weekly meetings based on newly acquired knowledge and team members’ preferences for different tasks. \n\n Our weekly meeting took place the day after each sprint ended (Thursday), so they were used as informal sprint retrospectives. We focused on clarifying and redefining the tasks for the coming sprint at these meetings, as well as assigning team members to those tasks. We also discussed any other non-task related items that we wanted to change for the next sprint. One main point of discussion was our Git branching strategy. The key factor of our branching strategy was the creation of a development branch (“dev” branch). For each sprint, we created a new dev branch off of main from which everyone was required to create their task branches off of. PRs and reviews therefore branched off of the dev branch, which granted us another level of separation between the code that was fit for deployment (main) and code that contained new features but hadn’t yet been cleared for deployment (dev branch). Before asking for reviews, we required PRs to come with detailed descriptions and passing tests. In order to merge a PR, at least two reviews were required. Often, we waited for all 3 reviews to ensure that all sprint members were seeing the code that was being added. In specific cases, where only a few lines had been edited, or two reviews were not possible in the desired timeframe, one reviewer was deemed sufficient. When PRs were opened outside of meeting times, asynchronous reviews were requested to increase review efficiency. In the asynchronous reviews, the PR creator gave an overview of decisions they made, either in the PR description or by adding Github comments on any relevant lines. We practiced blameless reviews, making sure to never discourage or blame anyone for decisions that we disagreed with or thought were suboptimal. It was very important to our group that we justified and discussed any non-trivial project decisions. \n\n In addition to discussing via PRs, our main method of communication was a text group chat. We used this chat to ping each other about scheduling, planning, PRs, reviews, and as a medium to hold more extensive and complicated discussions and decisions – which aren’t convenient on Github. \n\n To give more detail to our Kanban board process, we had a 5-column board with the following headings: Backlog, Next Sprint, In Progress, Group Review, and Completed. The headings are mostly self explanatory, but in Backlog we had the tasks that were initially defined, but not revised or ready for the current sprint. We also defined here any tasks that we could revisit if there was extra time - primarily refactoring tasks. Tasks were put into the Next Sprint section once they were discussed during our weekly meetings (and the corresponding task cards were rewritten to be more detailed). JIRA also allowed us to assign people specifically to tasks, which allowed us to conveniently organize and track task leads. \n\n As for the sprints, there were 4 sprints (0 through 3) defined for all groups, which we adjusted slightly. All sprints were scheduled to be two weeks, except for Sprint 1. Our team had mistakenly planned for the one-week sprint being Sprint 2, meaning we assigned approximately twice as many tasks for Sprint 1 and half as many for Sprint 2 than would be assigned with the intended durations. We reconciled this issue by keeping Sprint 1 as-is, but by creating ‘Sprint 1.5’ to bridge the differences of our duration breakdowns. Sprint 1 initially had eight tasks (two per person), which made it easy to divide it into two separate sprints."
    }
]